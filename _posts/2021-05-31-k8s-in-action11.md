---
title: 쿠버네티스 인 액션 - 11장 쿠버네티스 내부 이해
description: 쿠버네티스 인 액션 - 11장 쿠버네티스 내부 이해
categories:
 - dev
tags:
 - kubernetes
comments: true
---
> Kubernetes In Action 정리 - 쿠버네티스 내부 이해

  * 쿠버네티스 클러스터 구성 요소
  * 각 구성 요소의 기능과 동작 방법
  * 디플로이먼트 오브젝트를 생성해 파드를 실행하는 방법
  * 실행 중인 파드에 관하여
  * 파드 간의 네트워크 동작 방식
  * 쿠버네티스 서비스의 동작 방식
  * 고가용성 실현 방법

## 11.1 아키텍처 이해

쿠버네티스 클러스터를 이루는 구성 
* 쿠버네티스 컨트롤 플레인
* (워커) 노드

### 컨트롤 플레인 구성요소
* 컨트롤 플레인은 클러스터 기능을 제어 및 전체 클러스터가 동작하게 만드는 역할을 한다. 구성요소는 아래와 같다. 
  * etcd 분산 저장 스토리지
  * API 서버
  * 스케줄러
  * 컨트롤러 매니저

이들 구성 요소는 클러스터 상태를 저장 및 관리 하지만 애플리케이션 컨테이너를 직접 실행하는 것은 아니다. 

### 워커 노드에서 실행하는 구성요소
* kubelet
* 쿠버네티스 서비스 프록시 (kube-proxy)
* 컨테이너 런타임 (Dockerm rkt ...)

### 애드온 구성요소
컨트롤 플레인과 노드에서 실행되는 구성요소 이외에 몇가지 추가 구성요소가 필요.
* 쿠버네티스 DNS 서버
* 대시보드
* 인그레스 컨트롤러
* 힙스터
* 컨테이너 네트워크 인터페이스 플러그인

### 11.1.1 쿠버네티스 구성 요소의 분산 특성

![쿠버네티스 구성 요소](/assets/images/post/k8s/Kubernetes-101-Architecture-Diagram.jpeg)

```
컨트롤 플레인 구성 요소의 상태 확인
API 서버는 각 컨트롤 플레인 구성요소의 상태를 표시하는 ComponentStatus API 리소스 제공.
$ kubectl get componentstatus
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
etcd-0               Healthy     {"health":"true"}
```


### 구성 요소가 서로 통신하는 방법
쿠버네티스 시스템 구성요소는 오직 API 서버하고만 통신하며 API 서버는 etcd와 통신하는 유일한 구성요소이다.  
다른 구성요소는 etcd와 직접 통신하지 않고, API 서버로 클러스터 상태를 변경한다. (API 서버에 요청을 보낸다.)

### 개별 구성 요소의 여러 인스턴스 실행
워커 노드의 구성요소는 동일한 노드에서 실행되어야 하는 반면 컨트롤 플레인의 구성요소는 여러 서버에 걸쳐 실행 가능하다.  
각 컨트롤 플레인 구성 요소 인스턴스를 둘 이상 샐행하여 가용성을 높일 수 있다. (etcd, API 서버는 병렬 실행 가능, 스케줄러와 컨트롤러 매니저는 하나의 인스턴스만 활성화)

### 구성 요소 실행 방법
Kubelet은 항상 일반 시스템 구성 요소로 실행되는 유일한 구성요소며 Kubelet이 다른 구성 요소를 파드로 실행한다. (kube-proxy와 같은 구성요소는 파드로 실행할 수 있지만 kubelet은 시스템 구성요소(데몬)으로 실행되어야 한다.)  

> Kubelet 이란 ?  
클러스터의 각 노드에서 실행되는 에이전트. Kubelet은 파드에서 컨테이너가 확실하게 동작하도록 관리한다.  
Kubelet은 다양한 메커니즘을 통해 제공된 파드 스펙(PodSpec)의 집합을 받아서 컨테이너가 해당 파드 스펙에 따라 건강하게 동작하는 것을 확실히 한다. Kubelet은 쿠버네티스를 통해 생성되지 않는 컨테이너는 관리하지 않는다.  

```
kube-system 네임스페이스에 속한 파드 조회

Docker K8S
$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system
POD                                      NODE
etcd-docker-desktop                      docker-desktop
kube-apiserver-docker-desktop            docker-desktop
kube-controller-manager-docker-desktop   docker-desktop
kube-scheduler-docker-desktop            docker-desktop

사내 샌드박스 환경 
$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system
POD                                                        NODE
kube-controller-manager-dkosv3-hairshop-sandbox-master-1   dkosv3-hairshop-sandbox-master-1
cilium-operator-75dd7d444d-5sqmw                           dkosv3-hairshop-sandbox-master-1
nodelocaldns-5kdg7                                         dkosv3-hairshop-sandbox-master-1
coredns-5475448f89-cb4ss                                   dkosv3-hairshop-sandbox-master-1
kubernetes-metrics-scraper-74bc46ccdb-vl9wq                dkosv3-hairshop-sandbox-master-1
coredns-secondary-756cbdbd57-2f48q                         dkosv3-hairshop-sandbox-master-1
kubernetes-dashboard-55f55c7987-llk7s                      dkosv3-hairshop-sandbox-master-1
csi-cinder-controllerplugin-5b5f6f4f5f-65znq               dkosv3-hairshop-sandbox-master-1
kube-scheduler-dkosv3-hairshop-sandbox-master-1            dkosv3-hairshop-sandbox-master-1
dns-autoscaler-85f898cd5c-6kvvr                            dkosv3-hairshop-sandbox-master-1
dns-autoscaler-secondary-79b84d9867-9hp78                  dkosv3-hairshop-sandbox-master-1
kakao-cluster-manager-5txq6                                dkosv3-hairshop-sandbox-master-1
kakao-node-manager-bx87d                                   dkosv3-hairshop-sandbox-master-1
kube-proxy-wrnnx                                           dkosv3-hairshop-sandbox-master-1
kube-apiserver-dkosv3-hairshop-sandbox-master-1            dkosv3-hairshop-sandbox-master-1
cilium-fc77v                                               dkosv3-hairshop-sandbox-master-1
snapshot-controller-0                                      dkosv3-hairshop-sandbox-master-1
kube-proxy-nrqz7                                           dkosv3-hairshop-sandbox-worker-1
kakao-node-manager-nv6js                                   dkosv3-hairshop-sandbox-worker-1
coredns-secondary-756cbdbd57-427hs                         dkosv3-hairshop-sandbox-worker-1
coredns-5475448f89-x8ntn                                   dkosv3-hairshop-sandbox-worker-1
nginx-proxy-dkosv3-hairshop-sandbox-worker-1               dkosv3-hairshop-sandbox-worker-1
nodelocaldns-4tqfl                                         dkosv3-hairshop-sandbox-worker-1
cilium-tr8x5                                               dkosv3-hairshop-sandbox-worker-1
csi-cinder-nodeplugin-82kcd                                dkosv3-hairshop-sandbox-worker-1
```
컨트롤 플레인의 구성요소는 마스터노드에서 파드로 실행, 세 개의 워커 노드는 kube-proxy와 Flannel 파드를 실행한 오버레이 네트워크를 제공 (Docker K8S에서는 응답값이 다르다.)

### 11.1.2 쿠버네티스가 etcd를 사용하는 방법
영구적으로 저장되어야 하는 값들을 빠르고 분산하여 저장하기 위하여 일관된 key - value 저장소를 제공한다. 이것이 etcd이다.   
분산되어 있기 때문에 둘 이상의 etcd 인스턴스를 실행하여 고가용성과 우수한 성능 제공, 낙관적 잠금 시스템 및 유효성 검사 이점  
**쿠버네티스가 클러스터 상태와 메타데이터를 저장하는 유일한 장소가 etcd라는 것은 강조할 가치가 있다.**  

### 리소스를 etcd에 저장하는 방법
etcd는 파일 시스템과 유사하게 key - value 쌍을 만들어 저장한다. 쿠버네티스는 모든 데이터를 /registry 아래에 저장한다. 

```
$ etcdctl ls /registry
> Docker K8S 에서는 기본적으로 제공하지 않음.
키 목록이 리소스 형식에 해당한다. 

디렉터리 안에 있는 키 목록
$ etcdctl ls /registry/pods
> 네임 스페이스 단위로 저장된다. 

default 네임스페이스 안에 있는 파드의 etcd 항목
$ etcdctl ls /registry/pods/default

파드를 나타내는 etcd 항목
$ etcdctl get /registry/pods/default/kubia-xxx
{
  json 타입
}
```

파드를 나타내는 항목들은 json 형식으로 저장된다. 

### 저장된 오브젝트의 일관성과 유효성 보장
쿠버네티스는 모든 구성 요소가 API 서버를 통하도록 구현하여 데이터가 항상 일치 하도록 구현했다. API 서버 한곳에서 낙관적 잠금 메커니즘을 구현하여 클러스터의 상태를 업데이트 하고,  
오류가 발생할 가능성을 줄이고, 일관성을 가질수 있다. 

### 클러스터링된 etcd의 일관성 보장
고가용성을 보장하기 위해 두 개 이상의 etcd 인스턴스를 실행하는것이 일반적.  
etcd는 RAFT 합의 알고리즘을 사용하여 어느 순간이든 각 노드 상태가 대다수의 노드가 동의하는 현재 상태이거나, 이전에 동의된 상태 중에 하나임을 보장한다. 
합의 알고리즘은 클러스터가 다음 상태로 진행하기 위해 과반수가 필요하다. 

### etcd 인스턴스 수가 홀수인 이유
etcd는 인스턴스를 일반적으로 홀수로 배포한다. 하나의 인스턴스가 실패하는 경우 과반수의 인스턴스가 필요하기 때문이다. 

### 11.1.3 API 서버의 기능
쿠버네티스 API 서버는 다른 모든 구성 요소와 kubectl 같은 클라이언트에서 사용하는 중심 구성 요소이다.  
클러스터 상태를 조회 / 변경하기 위해 RESTful API를 제공한다. 변경된 상태는 etcd에 저장하고 오브젝트 유효성 검사 작업도 수행하기 때문에 잘못 설정된 오브젝트를 저장할 수 없다.  
API 서버의 클라이언트 중 하나는 kubectl 명령줄 도구이다. 


![API 서버의 동작](/assets/images/post/k8s/api-server.png)





* 이미지 출처
  * https://www.aquasec.com/cloud-native-academy/kubernetes-101/kubernetes-architecture/
  * https://kubernetes.io/ko/docs/concepts/security/controlling-access/