---
title: 쿠버네티스 인 액션 - 11장 쿠버네티스 내부 이해
description: 쿠버네티스 인 액션 - 11장 쿠버네티스 내부 이해
categories:
 - dev
tags:
 - kubernetes
comments: true
---
> Kubernetes In Action 정리 - 쿠버네티스 내부 이해

  * 쿠버네티스 클러스터 구성 요소
  * 각 구성 요소의 기능과 동작 방법
  * 디플로이먼트 오브젝트를 생성해 파드를 실행하는 방법
  * 실행 중인 파드에 관하여
  * 파드 간의 네트워크 동작 방식
  * 쿠버네티스 서비스의 동작 방식
  * 고가용성 실현 방법

## 11.1 아키텍처 이해

쿠버네티스 클러스터를 이루는 구성 
* 쿠버네티스 컨트롤 플레인
* (워커) 노드

### 컨트롤 플레인 구성요소
* 컨트롤 플레인은 클러스터 기능을 제어 및 전체 클러스터가 동작하게 만드는 역할을 한다. 구성요소는 아래와 같다. 
  * etcd 분산 저장 스토리지
  * API 서버
  * 스케줄러
  * 컨트롤러 매니저

이들 구성 요소는 클러스터 상태를 저장 및 관리 하지만 애플리케이션 컨테이너를 직접 실행하는 것은 아니다. 

### 워커 노드에서 실행하는 구성요소
* kubelet
* 쿠버네티스 서비스 프록시 (kube-proxy)
* 컨테이너 런타임 (Dockerm rkt ...)

### 애드온 구성요소
컨트롤 플레인과 노드에서 실행되는 구성요소 이외에 몇가지 추가 구성요소가 필요.
* 쿠버네티스 DNS 서버
* 대시보드
* 인그레스 컨트롤러
* 힙스터
* 컨테이너 네트워크 인터페이스 플러그인

### 11.1.1 쿠버네티스 구성 요소의 분산 특성

![쿠버네티스 구성 요소](/assets/images/post/k8s/Kubernetes-101-Architecture-Diagram.jpeg)

```
컨트롤 플레인 구성 요소의 상태 확인
API 서버는 각 컨트롤 플레인 구성요소의 상태를 표시하는 ComponentStatus API 리소스 제공.
$ kubectl get componentstatus
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused
etcd-0               Healthy     {"health":"true"}
```


### 구성 요소가 서로 통신하는 방법
쿠버네티스 시스템 구성요소는 오직 API 서버하고만 통신하며 API 서버는 etcd와 통신하는 유일한 구성요소이다.  
다른 구성요소는 etcd와 직접 통신하지 않고, API 서버로 클러스터 상태를 변경한다. (API 서버에 요청을 보낸다.)

### 개별 구성 요소의 여러 인스턴스 실행
워커 노드의 구성요소는 동일한 노드에서 실행되어야 하는 반면 컨트롤 플레인의 구성요소는 여러 서버에 걸쳐 실행 가능하다.  
각 컨트롤 플레인 구성 요소 인스턴스를 둘 이상 샐행하여 가용성을 높일 수 있다. (etcd, API 서버는 병렬 실행 가능, 스케줄러와 컨트롤러 매니저는 하나의 인스턴스만 활성화)

### 구성 요소 실행 방법
Kubelet은 항상 일반 시스템 구성 요소로 실행되는 유일한 구성요소며 Kubelet이 다른 구성 요소를 파드로 실행한다. (kube-proxy와 같은 구성요소는 파드로 실행할 수 있지만 kubelet은 시스템 구성요소(데몬)으로 실행되어야 한다.)  

> Kubelet 이란 ?  
클러스터의 각 노드에서 실행되는 에이전트. Kubelet은 파드에서 컨테이너가 확실하게 동작하도록 관리한다.  
Kubelet은 다양한 메커니즘을 통해 제공된 파드 스펙(PodSpec)의 집합을 받아서 컨테이너가 해당 파드 스펙에 따라 건강하게 동작하는 것을 확실히 한다. Kubelet은 쿠버네티스를 통해 생성되지 않는 컨테이너는 관리하지 않는다.  

```
kube-system 네임스페이스에 속한 파드 조회

Docker K8S
$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system
POD                                      NODE
etcd-docker-desktop                      docker-desktop
kube-apiserver-docker-desktop            docker-desktop
kube-controller-manager-docker-desktop   docker-desktop
kube-scheduler-docker-desktop            docker-desktop

사내 샌드박스 환경 
$ kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system
POD                                                        NODE
kube-controller-manager-dkosv3-hairshop-sandbox-master-1   dkosv3-hairshop-sandbox-master-1
cilium-operator-75dd7d444d-5sqmw                           dkosv3-hairshop-sandbox-master-1
nodelocaldns-5kdg7                                         dkosv3-hairshop-sandbox-master-1
coredns-5475448f89-cb4ss                                   dkosv3-hairshop-sandbox-master-1
kubernetes-metrics-scraper-74bc46ccdb-vl9wq                dkosv3-hairshop-sandbox-master-1
coredns-secondary-756cbdbd57-2f48q                         dkosv3-hairshop-sandbox-master-1
kubernetes-dashboard-55f55c7987-llk7s                      dkosv3-hairshop-sandbox-master-1
csi-cinder-controllerplugin-5b5f6f4f5f-65znq               dkosv3-hairshop-sandbox-master-1
kube-scheduler-dkosv3-hairshop-sandbox-master-1            dkosv3-hairshop-sandbox-master-1
dns-autoscaler-85f898cd5c-6kvvr                            dkosv3-hairshop-sandbox-master-1
dns-autoscaler-secondary-79b84d9867-9hp78                  dkosv3-hairshop-sandbox-master-1
kakao-cluster-manager-5txq6                                dkosv3-hairshop-sandbox-master-1
kakao-node-manager-bx87d                                   dkosv3-hairshop-sandbox-master-1
kube-proxy-wrnnx                                           dkosv3-hairshop-sandbox-master-1
kube-apiserver-dkosv3-hairshop-sandbox-master-1            dkosv3-hairshop-sandbox-master-1
cilium-fc77v                                               dkosv3-hairshop-sandbox-master-1
snapshot-controller-0                                      dkosv3-hairshop-sandbox-master-1
kube-proxy-nrqz7                                           dkosv3-hairshop-sandbox-worker-1
kakao-node-manager-nv6js                                   dkosv3-hairshop-sandbox-worker-1
coredns-secondary-756cbdbd57-427hs                         dkosv3-hairshop-sandbox-worker-1
coredns-5475448f89-x8ntn                                   dkosv3-hairshop-sandbox-worker-1
nginx-proxy-dkosv3-hairshop-sandbox-worker-1               dkosv3-hairshop-sandbox-worker-1
nodelocaldns-4tqfl                                         dkosv3-hairshop-sandbox-worker-1
cilium-tr8x5                                               dkosv3-hairshop-sandbox-worker-1
csi-cinder-nodeplugin-82kcd                                dkosv3-hairshop-sandbox-worker-1
```
컨트롤 플레인의 구성요소는 마스터노드에서 파드로 실행, 세 개의 워커 노드는 kube-proxy와 Flannel 파드를 실행한 오버레이 네트워크를 제공 (Docker K8S에서는 응답값이 다르다.)

### 11.1.2 쿠버네티스가 etcd를 사용하는 방법
영구적으로 저장되어야 하는 값들을 빠르고 분산하여 저장하기 위하여 일관된 key - value 저장소를 제공한다. 이것이 etcd이다.   
분산되어 있기 때문에 둘 이상의 etcd 인스턴스를 실행하여 고가용성과 우수한 성능 제공, 낙관적 잠금 시스템 및 유효성 검사 이점  
**쿠버네티스가 클러스터 상태와 메타데이터를 저장하는 유일한 장소가 etcd라는 것은 강조할 가치가 있다.**  

### 리소스를 etcd에 저장하는 방법
etcd는 파일 시스템과 유사하게 key - value 쌍을 만들어 저장한다. 쿠버네티스는 모든 데이터를 /registry 아래에 저장한다. 

```
$ etcdctl ls /registry
> Docker K8S 에서는 기본적으로 제공하지 않음.
키 목록이 리소스 형식에 해당한다. 

디렉터리 안에 있는 키 목록
$ etcdctl ls /registry/pods
> 네임 스페이스 단위로 저장된다. 

default 네임스페이스 안에 있는 파드의 etcd 항목
$ etcdctl ls /registry/pods/default

파드를 나타내는 etcd 항목
$ etcdctl get /registry/pods/default/kubia-xxx
{
  json 타입
}
```

파드를 나타내는 항목들은 json 형식으로 저장된다. 

### 저장된 오브젝트의 일관성과 유효성 보장
쿠버네티스는 모든 구성 요소가 API 서버를 통하도록 구현하여 데이터가 항상 일치 하도록 구현했다. API 서버 한곳에서 낙관적 잠금 메커니즘을 구현하여 클러스터의 상태를 업데이트 하고,  
오류가 발생할 가능성을 줄이고, 일관성을 가질수 있다. 

### 클러스터링된 etcd의 일관성 보장
고가용성을 보장하기 위해 두 개 이상의 etcd 인스턴스를 실행하는것이 일반적.  
etcd는 RAFT 합의 알고리즘을 사용하여 어느 순간이든 각 노드 상태가 대다수의 노드가 동의하는 현재 상태이거나, 이전에 동의된 상태 중에 하나임을 보장한다. 
합의 알고리즘은 클러스터가 다음 상태로 진행하기 위해 과반수가 필요하다. 

### etcd 인스턴스 수가 홀수인 이유
etcd는 인스턴스를 일반적으로 홀수로 배포한다. 하나의 인스턴스가 실패하는 경우 과반수의 인스턴스가 필요하기 때문이다. 

### 11.1.3 API 서버의 기능
쿠버네티스 API 서버는 다른 모든 구성 요소와 kubectl 같은 클라이언트에서 사용하는 중심 구성 요소이다.  
클러스터 상태를 조회 / 변경하기 위해 RESTful API를 제공한다. 변경된 상태는 etcd에 저장하고 오브젝트 유효성 검사 작업도 수행하기 때문에 잘못 설정된 오브젝트를 저장할 수 없다.  
API 서버의 클라이언트 중 하나는 kubectl 명령줄 도구이다. 


![API 서버의 동작](/assets/images/post/k8s/api-server.png)

### 인증 플러그인으로 클라이언트 인증
인증 방법에 따라 사용자를 클라이언트 인증서 혹은 HTTP 헤더에서 가져온다. 플러그인은 클라이언트의 사용자 이름, 사용자 ID, 속해있는 그룹 정보 추출

### 인가 플러그인을 통한 클라이언트 인가
API 서버는 하나 이상의 인가 플러그인을 사용하도록 되어 있다. 인증된 사용자가 요청한 작업이 수행 가능한 권한이 있는지를 판별  
ex) 파드를 생성할 때 API 서버는 모든 인가 플러그인을 호출하여 사용자가 요청한 네임스페이스안에 파드를 생성 할 수 있는지 결정

### 어드미션 컨트롤 플러그인으로 요청된 리소스 확인과 수정
리소스를 생성, 수정, 삭제 하려는 요청인 경우 어드미션 컨트롤로 보내진다. 이 플러그인은 리소스를 여러가지 이유로 수정할 수 있다. 리소스 정의에서 누락된 필드를 기본값으로 초기화 하거나 재정의 가능하다. 
* AlwaysPullImages : 파드의 imagePullPolicy를 Always로 변경하여 파드가 배포될 때 마다 이미지를 항상 강제로 가져오도록 재정의.
* ServiceAccount : 명시적으로 지정하지 않을 경우 default 서비스 어카운트를 적용
* NamespaceLifecycle : 삭제되는 과정에 있는 네임스페이스와 존재하지 않는 네임스페이스 안에 파드가 생성되는것을 방지
* ResourceQuota : 특정 네임스페이스 안에 있는 파드가 해당 네임스페이스에 할당된 CPU와 메모리만을 사용하도록 강제한다.

### 리소스 휴효성 확인 및 영구 저장
요청이 모든 어드미션 컨트롤 플러그인을 통과하면, API 서버는 오브젝트의 유효성을 검증하고 etcd에 저장한다. 

### 11.1.4 API 서버가 리소스 변경을 클라이언트에 통보하는 방법 이해
API 서버는 우리가 논의했던 것 외에 다른 아무것도 하지 않는다. API 서버는 컨트롤러에 무엇을 해야하는지 알려주지 않고 컨트롤러와 다른 구성 요소가 배포된 리소스의 변경사항을 관찰 할 수 있게 한다. 
컨트롤 플레인 구성 요소는 리소스가 생성, 수정, 삭제 될 떄 통보를 받을 수 있도록 요청할 수 있다.  
클라이언트는 API 서버에 HTTP 연결을 맺고 변경사항을 감지한다. 오브젝트가 변경되거나 갱신될 때 마다 서버는 연결된 모든 클라이언트에게 오브젝트의 새로운 버전을 보낸다.  
kubectl 도구를 사용하여 리소스 변경을 감시할 수 있다. 새로운 버전의 pod을 배포 하고, kubectl get pod -w 실행하면 감시 이벤트를 받을수 있다. 

### 11.1.5 스케줄러의 이해
일반적으로 파드를 실행할 때 클러스터 노드를 지정하지 않는 것을 알고 있다. 이것은 스케줄러에게 맡겨진 일이다. API 서버의 감시 메커니즘을 통해 새로운 파드가 생성되면 적절한 노드에게 파드를 할당한다.  
스케줄러는 선택된 노드에 파드를 실행하도록 지시 하지 않고 API 서버로 파드 정의를 갱신, API 서버는 kubelet에 새로운 파드가 스케줄링된것을 통보한다. 대상 노드의 kubelet이 파드의 컨테이너를 생성하고 실행한다. 

### 기본 스케줄링 알고리즘의 이해
* 모든 노드 중에서 파드를 스케줄링할 수 있는 노드 목록을 필터링
* 수용 가능한 노드의 우선순위를 정하고 점수가 높은 노드를 선택, 점수가 같은 경우 라운드 로빈을 사용

### 수용 가능한 노드 찾기
파드를 수용할 수 있는 노드를 찾기 위해 아래 조건들을 검사한다. 
* 노드가 하드웨어 리소스에 대한 파드 요청을 충족 시킬수 있는가 ? 
* 노드에 리소스가 부족한가 
* 파드를 특정 노드로 스케줄링하도록 요청한 경우 해당 노드인지 판별
* 노드가 파드 정의 안에 있는 노드 셀렉터와 일치하는 레이블을 가지고 있는가 ?
* 파드가 특정 호스트 포트에 할당 되도록 요청한 경우 해당 포트가 이 노드에서 이미 사용중인가 ?
* 파드 요청이 특정한 유형의 볼륨을 요청하는 경우 이 노드에서 해당 볼륨을 파드에 마운트 할 수 있는가, 아니면 이 노드에 있는 다른 파드가 이미 같은 볼륨을 사용하는가 ?
* 파드가 노드의 테인트를 허용하는가 ?
* 파드가 노드와ㅏ 파드의 어피니티, 안티 어피니티 규칙을 지정했는가 ? 

### 파드에 가장 적합한 노드 선택
두 개의 노드로 구성된 클러스터에서 A 노드가 10개의 파드를 실행, B 노드가 0개의 파드를 실행하고 있는 경우 새로운 파드는 B에 할당 될 것 이지만, A에 할당하여 B 노드를 반환하고 비용을 절감 시키는 방법도 있다. 
이러한 판단은 스케줄러가 해주지 않는다. 

### 고급 파드 스케줄링
파드의 레플리카가 여러개인 경우 한 노드에 스케줄링하는것 보다 많은 노드에 분산 시키는것이 효과적이다. 동일한 서비스 또는 레플리카셋에 속한 파드는 기본적으로 여러 노드에 분산된다. 

### 다중 스케줄러 사용
클러스터에서 여러개의 스케줄러를 실행할 수 있다. 파드 정의 안에 **schedulerName**속성에 파드를 스케줄링할 스케줄러를 지정할 수 있다. 속성을 지정하지 않을 경우 default-scheduler를 사용하여 스케줄링 된다.  
사용자가 직접 스케줄러를 구현하여 클러스터에 배포하거나, 다른 설정 옵션을 가진 쿠버네티스의 스케줄러를 배포할 수도 있다. 


### 11.1.6 컨트롤러 매니저에서 실행되는 컨트롤러 소개
API 서버는 리소스를 etcd에 저장하고 변경사항을 클라이언트에 통보한다. 스케줄러는 파드를 노드에 할당만 한다. API 서버에 배포된 리소스에 지정된 대로 시스템을 원하는 상태로 수렴되도록 하는 역할은 컨트롤러 매니저 안에서 실행되는 컨트롤러에 의해 수행된다. 
* 레플리케이션 매니저
* 레플리카셋, 데몬셋, 잡 컨트롤러
* 디플로이먼트 컨트롤러
* 스테이트풀셋 컨트롤러
* 노드 컨트롤러
* 서비스 컨트롤러
* 엔드포인트 컨트롤러
* 네임스페이스 컨트롤러
* 퍼시스턴트볼륨 컨트롤러
* ...

리소스는 클러스터에 어떤 것을 실행해야 하는지 기술하는 반면, 컨트롤러는 리소스를 배포함에 따라 실제 작업을 수행하는 활성화된 쿠버네티스 구성 요소이다. 

### 컨트롤러의 역할과 동작 방식 이해
컨트롤러는 다양한 작업을 수행하지만 API 서버에서 리소스 변경을 감지, 변경 작업을 수행한다. 일반적으로 컨트롤러는 조정 루프를 실행하여 리소스의 spec에 정의된 상태를 유지하려고 한다. 
컨트롤러는 API 서버에 연결하고 감시 메커니즘 컨트롤러가 담당하는 리소스 유형에서 변경이 발생하면 통보 해줄것을 요청한다. 


### 레플리케이션 매니저
레플리케이션컨트롤러 리소스를 활성화 하는 컨트롤러를 레플리케이션 매니저라고 한다. 컨트롤러는 파드 셀렉터와 일치하는 파드의 수를 찾고 이를 원하는(spec에 정의된) 레플리카 수와 비교한다. 
레플리카 수가 적거나 많으면 새로운 레플리케이션 매니저는 새로운 파드 매니페스트를 생성하여 API 서버에 게시하고 스케줄러와 kubelet이 작업을 수행한다. 





* 이미지 출처
  * https://www.aquasec.com/cloud-native-academy/kubernetes-101/kubernetes-architecture/
  * https://kubernetes.io/ko/docs/concepts/security/controlling-access/